Comment: >
  Run:
  python run.py trainer_cfg.fast_dev_run=1
  python run.py --config-name="ibl_paw" trainer_cfg.fast_dev_run=1
  python run.py trainer_cfg.fast_dev_run=1 module_cfg.module=cls_seq_bsoftmax

project_name: "data_ibl"
experiment_name: "run_ibl_multiclass"

data_cfg:
  test_set: "base"
  data_dir: ${oc.env:LOCAL_PROJECTS_DIR}/segment/action/data/ibl
  expt_ids:
    - cortexlab_KS020_2020-02-06-001
  #  - wittenlab_ibl_witten_26_2021-01-27-002
  input_type: "markers"
  input_size: 3  # TODO: update for other losses
  sequence_length: 7
  lambda_strong: 1
  lambda_weak: 1
  num_classes: 5
  batch_size: 32
  num_workers: 32
  class_idx: null
  seed: null
  encodings: null
  samples_per_class: null


trainer_cfg:
  fast_dev_run: 0
  logger: "wandb"
  deterministic: false
  log_every_n_steps: 1
  max_epochs: 500
  precision: 32
  accelerator: "auto"
  val_check_interval: 1.0  # set to 1 for early stopping
  gradient_clip_val: 0.5
  gradient_clip_algorithm: "value" #"norm"

eval_cfg:
  eval_only: 0
  ckpt_path: null
  return_model: 0  # debugger to return model

callbacks:
  gradnorm: 1
  checkpoint_callback: 0
  early_stopping: 1
  lr_monitor: 0   #this one?

early_stop_cfg:
  monitor: "epoch/val_accuracy"
  mode: max
  verbose: true
  min_delta: 0.0
  patience: 10  # related to val_check_interval

module_cfg:
  module: "cls_seq"
  samples_per_class: null  # inherit from train_dataloader
  class_idx: null # only if specific model
  classifier: "baseline"
  classifier_cfg:
    model_tier : "small"
    num_classes: null  # inherit from train_dataloader
    input_size: null  # inherit from train_dataloader
  optimizer_cfg:
    lr: 0.001



seed: 0

hydra:
  run:
    dir: ./outputs/${now:%y-%m-%d}/${now:%H-%M-%S}