Comment: >
  Debug:
  python run.py --config-name="fly_mingpt_v2" trainer_cfg.fast_dev_run=1 data_cfg.num_workers=4

project_name: "fly_mingpt_v2"
experiment_name: "min_gpt_long"

common:
  input_type: &input_type "markers"
  sequence_length: &sequence_length 500
  input_size: &input_size 16
  num_classes: &num_classes 7
  lambda_weak: &lambda_weak 0   # hyperparam on classifying weak (heuristic) labels
  lambda_strong: &lambda_strong 0.5  # hyperparam on classifying strong (hand) labels
  lambda_pred: &lambda_pred 1  # hyperparam on one-step-ahead prediction
  lambda_recon: &lambda_recon 0 # hyperparam on reconstruction loss
  lambda_task: &lambda_task 0 # hyperparam on task prediction loss


data_cfg:
  test_set: "concat_all" # "concat_all" or "base" for many or 1 data loaders
  data_dir: ${oc.env:LOCAL_PROJECTS_DIR}/segment/action/data/fly
  expt_ids:
    - 2019_08_07_fly2
    - 2019_08_08_fly1
    - 2019_08_20_fly2
    - 2019_10_10_fly3
    - 2019_10_14_fly3
  ood_expt_ids:
    - 2019_06_26_fly2
    - 2019_08_14_fly1
    - 2019_08_20_fly3
    - 2019_10_14_fly2
    - 2019_10_21_fly1
  batch_size: 2
  num_workers: 32
  seed: null
  train_split : 0.8
  val_split: 0.1
  input_type: *input_type
  input_size: *input_size  # dimensionality of input type
  sequence_length: *sequence_length
  num_classes: *num_classes
  lambda_weak: *lambda_weak
  lambda_strong: *lambda_strong
  lambda_pred: *lambda_pred
  lambda_recon: *lambda_recon
  lambda_task: *lambda_task

#  sequence_pad: null # inherited from dataloaderq
# class_idx: null
#  encodings: null
#   samples_per_class: null

trainer_cfg:
  fast_dev_run: 0
  logger: "wandb"
  deterministic: false
  log_every_n_steps: 1
  max_epochs: 100
  precision: 32
  accelerator: "auto"
  val_check_interval: 1.0  # set to 1 for early stopping
  gradient_clip_val: 0.1
  gradient_clip_algorithm: "norm" #"norm"


eval_cfg:
  eval_only: 0   # run evaluation only pipeline
  ckpt_path: null  # path to checkpoint
  return_model: 0  # debugger to return model

callbacks:
  gradnorm: 1
  checkpoint_callback: 0
  early_stopping: 1
  lr_monitor: 0

early_stop_cfg:
  monitor: "epoch/val_mse_pred"# "epoch/val_accuracy"
  mode: min  # max
  verbose: true
  min_delta: 0.0
  patience: 10  # related to val_check_interval

module_cfg:
  module: "segmenter_module"  # lightning module
  input_type: *input_type
  lambda_strong: *lambda_strong
  lambda_weak: *lambda_weak
  lambda_pred: *lambda_pred
  lambda_recon: *lambda_recon
  lambda_task: *lambda_task
  optimizer_cfg:
    type: "adamw"
    lr: !!float 1e-5
    weight_decay: 0.1
    betas: [0.9, 0.95]
  classifier: "encoder_decoder"  # model_class
  classifier_cfg:
    model_class: null
    backbone : "animalst"
    n_embd: 768      # hidden units per hidden layer
    n_hid_layers: 24      # hidden layers in network
    n_head: 12      # number of attention blocks
    n_hid_units: 256  # size of encoder output

    embd_pdrop: 0   # embedding dropout
    resid_pdrop: 0  # residual dropout
    attn_pdrop: 0  # attention, mlp dropout

    num_animals: 1
    classifier_type: "multiclass"

    num_frames: *sequence_length
    input_size: *input_size
    num_classes: *num_classes

    #lambda_strong: *lambda_strong
    #lambda_weak: *lambda_weak
    #lambda_pred: *lambda_pred
    #lambda_recon: *lambda_recon

seed: 0

fiftyone_cfg:
  dataset_name: "fly_daart"

hydra:
  run:
    dir: ./outputs/${now:%y-%m-%d}/${now:%H-%M-%S}