Comment: >
  # Debugging instructions:
  python run.py --config-name="killifish" trainer_cfg.fast_dev_run=1 data_cfg.num_workers=4

project_name: "killifish_mingpt_v2"
experiment_name: "killifish_baseline"

common:
  input_type: &input_type "markers"
  sequence_length: &sequence_length 144 # sequence length of each batch
  input_size: &input_size 100   # input dimension of data
  num_classes: &num_classes 0
  lambda_weak: &lambda_weak 0   # hyperparam on classifying weak (heuristic) labels
  lambda_strong: &lambda_strong 0  # hyperparam on classifying strong (hand) labels
  lambda_pred: &lambda_pred 1  # hyperparam on one-step-ahead prediction
  lambda_recon: &lambda_recon 0 # hyperparam on reconstruction loss
  lambda_task: &lambda_task 0 # hyperparam on reconstruction loss


data_cfg:
  test_set: "concat_all" # "concat_all" or "base" for many or 1 data loaders
  data_dir: ${oc.env:LOCAL_PROJECTS_DIR}/segment/action/data/killifish
  expt_ids:
    - fish0_137_cohort1
  ood_expt_ids:
    - fish0_137_cohort1
  batch_size: 2
  num_workers: 32
  seed: null
  train_split : 0.8
  val_split: 0.1
  input_type: *input_type
  input_size: *input_size  # dimensionality of input type
  sequence_length: *sequence_length
  num_classes: *num_classes
  normalize_markers: 'sum_norm_adim'
  lambda_weak: *lambda_weak
  lambda_strong: *lambda_strong
  lambda_pred: *lambda_pred
  lambda_recon: *lambda_recon


trainer_cfg:
  fast_dev_run: 0
  logger: "wandb"
  deterministic: false
  log_every_n_steps: 1
  max_epochs: 100
  precision: 32
  accelerator: "auto"
  val_check_interval: 1.0  # set to 1 for early stopping
  gradient_clip_val: 0.1
  gradient_clip_algorithm: "norm" #"norm"


eval_cfg:
  eval_only: 0   # run evaluation only pipeline
  ckpt_path: null  # path to checkpoint
  return_model: 0  # debugger to return model

callbacks:
  gradnorm: 1
  checkpoint_callback: 0
  early_stopping: 1
  lr_monitor: 0

early_stop_cfg:
  monitor: "epoch/val_mse_pred" # "epoch/val_accuracy"
  mode: min  # max
  verbose: true
  min_delta: 0.0
  patience: 10  # related to val_check_interval

module_cfg:
  module: "segmenter_module"  # lightning module
  input_type: *input_type
  lambda_strong: *lambda_strong
  lambda_weak: *lambda_weak
  lambda_pred: *lambda_pred
  lambda_recon: *lambda_recon
  lambda_task: *lambda_task
  optimizer_cfg:
    type: "adamw"
    lr: !!float 1e-5
    weight_decay: 0.1
    betas: [0.9, 0.95]
  classifier: "encoder_decoder"  # model class
  classifier_cfg:
    model_class: "deep"
    backbone : "animalst"
    n_embd: 768      # hidden units per hidden layer
    n_hid_layers: 24      # hidden layers in network
    n_head: 12      # number of attention blocks
    n_hid_units: 256  # size of encoder output

    embd_pdrop: 0   # embedding dropout
    resid_pdrop: 0  # residual dropout
    attn_pdrop: 0  # attention, mlp dropout
    num_animals: 1
    classifier_type: "multiclass"
    variational: false
    num_frames: *sequence_length



seed: 0

fiftyone_cfg:
  dataset_name: "fly_daart"

hydra:
  run:
    dir: ./outputs/${now:%y-%m-%d}/${now:%H-%M-%S}